{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyI67Vu49Nh1"
      },
      "source": [
        "# Quantization of pre-trained Image Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR2odAzTxmq6"
      },
      "source": [
        "This notebook tries to establish a TIME and SPACE baseline with different quantization techiques.\n",
        "\n",
        "* [SegFormer Part 3, Quantization Description](https://qte77.github.io/SegFormer-Part3-Quantization-Description/)\n",
        "* [SegFormer Part 4, Quantization Difficulties and Errors Part 1](https://qte77.github.io/SegFormer-Part4-Quantization-Difficulties-Part1/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ0IIC4J3std"
      },
      "source": [
        "# Prelims"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  google_colab = True\n",
        "except ImportError:\n",
        "  from os.path import basename\n",
        "  google_colab = False"
      ],
      "metadata": {
        "id": "VQCPMQJv_coB"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0h5ixf4gtpn"
      },
      "source": [
        "## Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "E5genFq6GTXm"
      },
      "outputs": [],
      "source": [
        "project_name = \"SegFormer\"\n",
        "nb_name = \"hf_segformer_quant_PoC.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "5WMEawzyCEyG"
      },
      "outputs": [],
      "source": [
        "model_checkpoints = {\n",
        "    # \"swintiny\": \"microsoft/swin-tiny-patch4-window7-224\", # not implemented\n",
        "    # \"b0ade\": \"nvidia/segformer-b0-finetuned-ade-512-512\", # not implemented\n",
        "    \"b0\": \"nvidia/mit-b0\"\n",
        "}\n",
        "# tasks = [\"classification\", \"segmentation\"] # not implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "TvesRb1cGtWq"
      },
      "outputs": [],
      "source": [
        "dataset_url = \"http://madm.dfki.de/files/sentinel/EuroSAT.zip\"\n",
        "ds_num_shards = 1000 # higher faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "p9dMyptFGLdh"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "num_train_epochs = 10\n",
        "metric_to_load = \"accuracy\" # accuracy for cls, mean_iou for seg\n",
        "test_calculation_speed = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Param prep"
      ],
      "metadata": {
        "id": "pIKCIRiSAcUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = model_checkpoints['b0']\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "dataset_name = dataset_url.split(\"/\")[-1].split(\".\")[0]"
      ],
      "metadata": {
        "id": "zeTSq7RL_lvE"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "if google_colab:\n",
        "  MNT_PATH = Path(\"/content/drive\")\n",
        "  SAVE_PATH = MNT_PATH / \"MyDrive\"\n",
        "  NB_PATH = SAVE_PATH / \"Colab Notebooks\"\n",
        "  nb_name = \"hf_segformer_quant_PoC.ipynb\"\n",
        "else:\n",
        "  SAVE_PATH = Path(\".\")\n",
        "  NB_PATH = Path(\".\")\n",
        "  nb_name = basename(__file__)"
      ],
      "metadata": {
        "id": "hM-oLGL5AllD"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "tAVfROHNGyXK"
      },
      "outputs": [],
      "source": [
        "NB_PATH = NB_PATH / f\"{project_name}/PoC\"\n",
        "DS_PATH = SAVE_PATH / f\"Datasets/{dataset_name}\"\n",
        "MODEL_PATH = SAVE_PATH / f\"Models/{model_checkpoint}\"\n",
        "TOK_PATH = SAVE_PATH / f\"Tokenizer/{model_checkpoint}\"\n",
        "PY_PATH = NB_PATH / \"utils\"\n",
        "REQS_PATH = NB_PATH / \"reqs\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if google_colab: drive.mount(str(MNT_PATH))\n",
        "NB_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgDMowNFBdUQ",
        "outputId": "bd686082-421e-46c9-fb8b-379d2cf84bf6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2JhoiZqgvIK"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''reimport modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98HIjbuRJF2G",
        "outputId": "4323e714-fd93-40b6-e7a1-7475ca50c4a1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reimport modules\\n%load_ext autoreload\\n%autoreload 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "lM3JBl7FTduS"
      },
      "outputs": [],
      "source": [
        "for req in [\"requirements_quant.txt\"]: #, \"requirements_jupymill.txt\"]:\n",
        "  req_file = REQS_PATH / req\n",
        "  assert req_file.exists(), \"Not Again!\"\n",
        "  %pip install -qqr \"{req_file}\" -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "wqVYtuJKvFq_"
      },
      "outputs": [],
      "source": [
        "# huggingface\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from evaluate import load\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from transformers import (\n",
        "  AutoImageProcessor,\n",
        "  # SegformerForSemanticSegmentation,\n",
        "  SegformerForImageClassification,\n",
        "  TrainingArguments,\n",
        "  Trainer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "PsfJ0b4zsQdC"
      },
      "outputs": [],
      "source": [
        "# tensor\n",
        "from numpy import argmax\n",
        "from torchinfo import summary\n",
        "from torch import (\n",
        "    ge, zeros,\n",
        "    stack, tensor, where,\n",
        "    device, no_grad # autograd.grad_mode\n",
        ")\n",
        "# from torch.optim import AdamW\n",
        "from torchvision.transforms import (\n",
        "  CenterCrop,\n",
        "  Compose,\n",
        "  Normalize,\n",
        "  RandomHorizontalFlip,\n",
        "  RandomResizedCrop,\n",
        "  Resize,\n",
        "  ToTensor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "VEDWAPDWrdLY"
      },
      "outputs": [],
      "source": [
        "# quantization\n",
        "import accelerate\n",
        "import bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "jNBn8O-7-2i6"
      },
      "outputs": [],
      "source": [
        "from random import randrange\n",
        "from os import chdir # , listdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wuAD2SU918u"
      },
      "source": [
        "### Custom utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "grDmO2xn3F7E"
      },
      "outputs": [],
      "source": [
        "chdir(PY_PATH)\n",
        "from utils import (\n",
        "    get_device_info, get_calculation_speed,\n",
        "    get_split_dataset, get_image_processor,\n",
        "    get_model_as_dict, print_models_stats\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqMh86n13gTW"
      },
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "P8RB93D-3NN1"
      },
      "outputs": [],
      "source": [
        "deviceinfo = get_device_info()\n",
        "is_cuda = deviceinfo[\"devicename\"].type == \"cuda\"\n",
        "# assert is_cuda, \"GPU needed for quantization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPcvHrz9N2e9",
        "outputId": "cf2da7ff-dfe0-4385-a41a-2e200d744a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu max memory: {'cpu': '12GiB'}\n",
            "Tue Jun 11 22:48:55 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0              33W /  70W |    167MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "cuda allocated memory: 14\n",
            "cuda max memory: {0: '14GiB'}\n"
          ]
        }
      ],
      "source": [
        "print(\"cpu max memory: %s\" % deviceinfo[\"cpu_max_memory\"])\n",
        "if is_cuda:\n",
        "  !nvidia-smi\n",
        "  print(\"cuda allocated memory: %s\" % deviceinfo[\"cuda_free_mem\"])\n",
        "  print(\"cuda max memory: %s\" % deviceinfo[\"cuda_max_memory\"])\n",
        "else:\n",
        "  print(\"GPU needed for quantization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtLMX51KDgt4"
      },
      "source": [
        "## Test calculation speed of dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfjHE85VMsFQ",
        "outputId": "d1a18171-3918-4f54-8743-2d91cfed46dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev='cuda'\n",
            "torch.float64\n",
            "mean: 0.002898, stdev: 0.001178\n",
            "torch.float32\n",
            "mean: 0.001703, stdev: 0.000580\n",
            "torch.float16\n",
            "mean: 0.002524, stdev: 0.002195\n",
            "torch.bfloat16\n",
            "mean: 0.002466, stdev: 0.000640\n",
            "torch.int8\n",
            "mean: 0.002611, stdev: 0.001402\n",
            "torch.uint8\n",
            "mean: 0.002033, stdev: 0.000260\n",
            "torch.bool\n",
            "mean: 0.003670, stdev: 0.000631\n",
            "dev='cpu'\n",
            "torch.float64\n",
            "mean: 0.132278, stdev: 0.071404\n",
            "torch.float32\n",
            "mean: 0.087363, stdev: 0.021172\n",
            "torch.float16\n",
            "mean: 1.551432, stdev: 0.505957\n",
            "torch.bfloat16\n",
            "mean: 0.039436, stdev: 0.006567\n",
            "torch.int8\n",
            "mean: 0.419538, stdev: 0.050901\n",
            "torch.uint8\n",
            "mean: 0.672995, stdev: 0.228865\n",
            "torch.bool\n",
            "mean: 1.731381, stdev: 0.610707\n"
          ]
        }
      ],
      "source": [
        "# https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype\n",
        "if test_calculation_speed:\n",
        "  timings = get_calculation_speed(deviceinfo[\"devicename\"].type, tee=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RejTegpKW6pe"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "c8b9EfVfiSjv"
      },
      "outputs": [],
      "source": [
        "dataset = get_split_dataset(str(DS_PATH), dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D3I1wcGlH-l",
        "outputId": "2b557ebc-b0c4-4ca5-e7a5-b5b2bf7bcee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': Dataset({\n",
              "     features: ['image', 'label'],\n",
              "     num_rows: 24300\n",
              " }),\n",
              " 'test': Dataset({\n",
              "     features: ['image', 'label'],\n",
              "     num_rows: 2700\n",
              " })}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnnL3yHBI7Z3",
        "outputId": "c85bcf6b-0af2-43c7-fdac-2e1b21a4a017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': Image(mode=None, decode=True, id=None),\n",
              " 'label': ClassLabel(names=['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'], id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "dataset[\"train\"].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BujWoSgyMQlw",
        "outputId": "ebb93983-4e6b-40e3-9d9b-83e2b32c6592"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>,\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "example = dataset[\"train\"][10]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "32iolZyTkNlI",
        "outputId": "00e67342-80a1-4f5c-b43c-ba72b586beda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "example['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdTU9jGCxwFr",
        "outputId": "103686e4-9f50-486c-a7e9-592f310fe865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "example['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxoikSOjs0K"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1bX4lGAO_d9",
        "outputId": "fe311ad2-99a8-4aa4-ea18-37dbaa0d3c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from /content/drive/MyDrive/Tokenizer/nvidia/mit-b0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/segformer/image_processing_segformer.py:103: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SegformerImageProcessor {\n",
              "  \"_valid_processor_keys\": [\n",
              "    \"images\",\n",
              "    \"segmentation_maps\",\n",
              "    \"do_resize\",\n",
              "    \"size\",\n",
              "    \"resample\",\n",
              "    \"do_rescale\",\n",
              "    \"rescale_factor\",\n",
              "    \"do_normalize\",\n",
              "    \"image_mean\",\n",
              "    \"image_std\",\n",
              "    \"do_reduce_labels\",\n",
              "    \"return_tensors\",\n",
              "    \"data_format\",\n",
              "    \"input_data_format\"\n",
              "  ],\n",
              "  \"do_normalize\": true,\n",
              "  \"do_reduce_labels\": false,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.485,\n",
              "    0.456,\n",
              "    0.406\n",
              "  ],\n",
              "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
              "  \"image_std\": [\n",
              "    0.229,\n",
              "    0.224,\n",
              "    0.225\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"height\": 512,\n",
              "    \"width\": 512\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "image_processor = get_image_processor(model_checkpoint, TOK_PATH)\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "4O_p3WrpRyej"
      },
      "outputs": [],
      "source": [
        "# on-the-fly image preparation and augmentation with <dataset>.set_transform(<transform_fn>)\n",
        "# https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "if \"height\" in image_processor.size:\n",
        "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "    crop_size = size\n",
        "    max_size = None\n",
        "elif \"shortest_edge\" in image_processor.size:\n",
        "    size = image_processor.size[\"shortest_edge\"]\n",
        "    crop_size = (size, size)\n",
        "    max_size = image_processor.size.get(\"longest_edge\")\n",
        "\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            RandomResizedCrop(crop_size),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            Resize(size),\n",
        "            CenterCrop(crop_size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "dataset[\"train\"].set_transform(preprocess_train)\n",
        "dataset[\"test\"].set_transform(preprocess_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng9TAlDV8d7r",
        "outputId": "e8b08d51-4ff0-4a08-9d7c-f048222dc471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['image', 'label', 'pixel_values'])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "dataset[\"train\"][0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Flu05Cat0lH"
      },
      "source": [
        "## Ids and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLhvKF3HtixI",
        "outputId": "282736bd-bce1-4964-dce4-bd943f619172"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'AnnualCrop',\n",
              " 1: 'Forest',\n",
              " 2: 'HerbaceousVegetation',\n",
              " 3: 'Highway',\n",
              " 4: 'Industrial',\n",
              " 5: 'Pasture',\n",
              " 6: 'PermanentCrop',\n",
              " 7: 'Residential',\n",
              " 8: 'River',\n",
              " 9: 'SeaLake'}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "id2label = {k:v for k,v in enumerate(labels)}\n",
        "label2id = {v:k for k,v in enumerate(labels)}\n",
        "id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6FYt16Bs3W7"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "8UGse36eLeeb"
      },
      "outputs": [],
      "source": [
        "metric = load(metric_to_load)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "EVWfiBuv2uCS"
      },
      "outputs": [],
      "source": [
        "# the compute_metrics function takes a Named Tuple as input:\n",
        "# predictions, which are the logits of the model as Numpy arrays,\n",
        "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
        "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = argmax(eval_pred.predictions, axis=1)\n",
        "    return  metric.compute(\n",
        "      predictions=predictions,\n",
        "      references=eval_pred.label_ids\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PccZO_PtgWn"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0YbSq3L9zof"
      },
      "source": [
        "## Load versions for QAT and compare SPACE/TIME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKPDnGXvSP-R"
      },
      "source": [
        "### 8-bit quantization with bitsandbytes\n",
        "\n",
        "From [LLM.int8() Paper](https://arxiv.org/abs/2110.02861), [Source GH](https://github.com/TimDettmers/bitsandbytes).\n",
        "[8-bit HF inference example](https://github.com/TimDettmers/bitsandbytes/blob/main/examples/int8_inference_huggingface.py)\n",
        "\n",
        "* optimizer\n",
        "  * `bnb.optim.Adam8bit(....)`\n",
        "  * `bnb.nn.Embedding(..)`\n",
        "* inference\n",
        "  * `linear = bnb.nn.Linear8bitLt(...)`\n",
        "  * Modes: mixed-precision, int8\n",
        "  * or full `LLM.int8()` method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tV23YL0NygQ"
      },
      "source": [
        "`BitsAndBytesConfig` also offers configuration support.\n",
        "\n",
        "```python\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=bf16)\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "hEfwMQ9BBDZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4885db1-2ffe-4f45-f0eb-5cb07c0e80c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_base': {'params': {}},\n",
              " 'model_half': {'params': {'half': True}},\n",
              " 'model_int8': {'params': {'load_in_8bit': True}},\n",
              " 'model_int4': {'params': {'load_in_4bit': True}}}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "models = {\"model_base\": {\"params\": {}}}\n",
        "if is_cuda:\n",
        "  models.update({\n",
        "    \"model_half\": {\"params\": {\"half\": True}},\n",
        "    \"model_int8\": {\"params\": {\"load_in_8bit\": True}},\n",
        "    \"model_int4\": {\"params\": {\"load_in_4bit\": True}}\n",
        "  })\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deviceinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWNoCP1CLxde",
        "outputId": "79e4ccc6-adc5-4d8a-8c41-794dcb747eb2"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'devicename': device(type='cuda'),\n",
              " 'cpu_max_memory': {'cpu': '12GiB'},\n",
              " 'cuda_free_mem': 14,\n",
              " 'cuda_n_gpus': 1,\n",
              " 'cuda_max_memory': {0: '14GiB'}}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if is_cuda:\n",
        "  max_memory = {}\n",
        "  for k,v in {\n",
        "      **deviceinfo[\"cpu_max_memory\"],\n",
        "      **deviceinfo[\"cuda_max_memory\"]\n",
        "    }.items():\n",
        "    v = int(v.replace(\"GiB\", \"\"))-2\n",
        "    max_memory[k] = f\"{v}GiB\"\n",
        "  max_memory"
      ],
      "metadata": {
        "id": "W-W1V4huul7I"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "HOGcbq-NK3T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f23c89-87ea-4f08-9cea-6ce93c84bf62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model_base: {'params': {}}\n",
            "OrderedDict([('', 0)])\n",
            "dict_keys(['load_time', 'model', 'memory_footprint'])\n",
            "\n",
            "model_half: {'params': {'half': True}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('', 0)])\n",
            "dict_keys(['load_time', 'model', 'memory_footprint'])\n",
            "\n",
            "model_int8: {'params': {'load_in_8bit': True}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('', 0)])\n",
            "dict_keys(['load_time', 'model', 'memory_footprint'])\n",
            "\n",
            "model_int4: {'params': {'load_in_4bit': True}}\n",
            "OrderedDict([('', 0)])\n",
            "dict_keys(['load_time', 'model', 'memory_footprint'])\n"
          ]
        }
      ],
      "source": [
        "# get models\n",
        "#if is_cuda:\n",
        "for k,v in models.items():\n",
        "  print(f\"\\n{k}: {v}\")\n",
        "  models[k] = get_model_as_dict(\n",
        "    model_checkpoint, MODEL_PATH,\n",
        "    max_memory, v[\"params\"]\n",
        "  )\n",
        "  print(\n",
        "    accelerate.infer_auto_device_map(models[k][\"model\"], max_memory=max_memory)\n",
        "  )\n",
        "  print(f\"{models[k].keys()}\")\n",
        "#else:\n",
        "#  model = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "6GnAEkRtomkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f775f3f8-62fd-46ef-881d-8dc3bf6fb09b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_base': {'load_time': 1.4295863460001783,\n",
              "  'memory_footprint': 14305568,\n",
              "  'is_quantized': None,\n",
              "  'device_map': {'': device(type='cuda', index=0)}},\n",
              " 'model_half': {'load_time': 1.2120439969999097,\n",
              "  'memory_footprint': 7152784,\n",
              "  'is_quantized': True,\n",
              "  'device_map': {'': device(type='cuda', index=0)}},\n",
              " 'model_int8': {'load_time': 0.9208884869999565,\n",
              "  'memory_footprint': 4842640,\n",
              "  'is_quantized': True,\n",
              "  'device_map': {'': device(type='cuda', index=0)}},\n",
              " 'model_int4': {'load_time': 1.3591602530000273,\n",
              "  'memory_footprint': 3687568,\n",
              "  'is_quantized': True,\n",
              "  'device_map': {'': device(type='cuda', index=0)}}}"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "models_stats = {}\n",
        "for k, v in models.items():\n",
        "  try:\n",
        "    is_quantized = models[k]['model'].is_quantized\n",
        "  except:\n",
        "    is_quantized = None\n",
        "  models_stats[k] = {\n",
        "    \"load_time\": v[\"load_time\"],\n",
        "    \"memory_footprint\": v[\"memory_footprint\"],\n",
        "    \"is_quantized\": is_quantized,\n",
        "    \"device_map\": v[\"model\"].hf_device_map\n",
        "  }\n",
        "models_stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['model_base']['model'].config.transformers_version"
      ],
      "metadata": {
        "id": "PywVhuR5SICo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad0769a4-1cad-4a5d-f53c-e3f1984654ce"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.12.0.dev0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install git+https://github.com/mert-kurttutan/torchview -qq\n",
        "model_graph = draw_graph(models['model_base']['model'])\n",
        "model_graph.visual_graph\n",
        "'''"
      ],
      "metadata": {
        "id": "n1pFM2kVdxIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9475e655-f51e-464c-b00e-f5dc9218961c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!pip install git+https://github.com/mert-kurttutan/torchview -qq\\nmodel_graph = draw_graph(models['model_base']['model'])\\nmodel_graph.visual_graph\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['model_base']['model'].__dir__()"
      ],
      "metadata": {
        "id": "i7bo-4IBVNuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0419712d-d648-4907-f3a5-5214c7fa7ac9"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T_destination',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_apply',\n",
              " '_assisted_decoding',\n",
              " '_auto_class',\n",
              " '_autoset_attn_implementation',\n",
              " '_backward_compatibility_gradient_checkpointing',\n",
              " '_backward_hooks',\n",
              " '_backward_pre_hooks',\n",
              " '_beam_sample',\n",
              " '_beam_search',\n",
              " '_buffers',\n",
              " '_call_impl',\n",
              " '_check_and_enable_flash_attn_2',\n",
              " '_check_and_enable_sdpa',\n",
              " '_compiled_call_impl',\n",
              " '_constrained_beam_search',\n",
              " '_contrastive_search',\n",
              " '_convert_head_mask_to_5d',\n",
              " '_copy_lm_head_original_to_resized',\n",
              " '_create_repo',\n",
              " '_dispatch_accelerate_model',\n",
              " '_expand_inputs_for_generation',\n",
              " '_extract_past_from_model_output',\n",
              " '_forward_hooks',\n",
              " '_forward_hooks_always_called',\n",
              " '_forward_hooks_with_kwargs',\n",
              " '_forward_pre_hooks',\n",
              " '_forward_pre_hooks_with_kwargs',\n",
              " '_from_config',\n",
              " '_get_backward_hooks',\n",
              " '_get_backward_pre_hooks',\n",
              " '_get_candidate_generator',\n",
              " '_get_decoder_start_token_id',\n",
              " '_get_files_timestamps',\n",
              " '_get_initial_cache_position',\n",
              " '_get_logits_processor',\n",
              " '_get_logits_warper',\n",
              " '_get_name',\n",
              " '_get_no_split_modules',\n",
              " '_get_resized_embeddings',\n",
              " '_get_resized_lm_head',\n",
              " '_get_static_cache',\n",
              " '_get_stopping_criteria',\n",
              " '_greedy_search',\n",
              " '_group_beam_search',\n",
              " '_has_unfinished_sequences',\n",
              " '_hf_peft_config_loaded',\n",
              " '_hook_rss_memory_post_forward',\n",
              " '_hook_rss_memory_pre_forward',\n",
              " '_init_weights',\n",
              " '_initialize_weights',\n",
              " '_is_full_backward_hook',\n",
              " '_is_hf_initialized',\n",
              " '_is_quantized_training_enabled',\n",
              " '_keep_in_fp32_modules',\n",
              " '_keep_in_fp32_modules',\n",
              " '_keys_to_ignore_on_load_missing',\n",
              " '_keys_to_ignore_on_load_unexpected',\n",
              " '_keys_to_ignore_on_save',\n",
              " '_load_from_state_dict',\n",
              " '_load_pretrained_model',\n",
              " '_load_pretrained_model_low_mem',\n",
              " '_load_state_dict_post_hooks',\n",
              " '_load_state_dict_pre_hooks',\n",
              " '_maybe_initialize_input_ids_for_generation',\n",
              " '_maybe_warn_non_full_backward_hook',\n",
              " '_merge_criteria_processor_list',\n",
              " '_modules',\n",
              " '_named_members',\n",
              " '_no_split_modules',\n",
              " '_non_persistent_buffers_set',\n",
              " '_parameters',\n",
              " '_prepare_attention_mask_for_generation',\n",
              " '_prepare_decoder_input_ids_for_generation',\n",
              " '_prepare_encoder_decoder_kwargs_for_generation',\n",
              " '_prepare_generated_length',\n",
              " '_prepare_generation_config',\n",
              " '_prepare_model_inputs',\n",
              " '_prepare_special_tokens',\n",
              " '_register_load_state_dict_pre_hook',\n",
              " '_register_state_dict_hook',\n",
              " '_reorder_cache',\n",
              " '_replicate_for_data_parallel',\n",
              " '_resize_token_embeddings',\n",
              " '_sample',\n",
              " '_save_to_state_dict',\n",
              " '_set_default_torch_dtype',\n",
              " '_set_gradient_checkpointing',\n",
              " '_skip_keys_device_placement',\n",
              " '_slow_forward',\n",
              " '_state_dict_hooks',\n",
              " '_state_dict_pre_hooks',\n",
              " '_supports_cache_class',\n",
              " '_supports_flash_attn_2',\n",
              " '_supports_sdpa',\n",
              " '_supports_static_cache',\n",
              " '_temporary_reorder_cache',\n",
              " '_tie_encoder_decoder_weights',\n",
              " '_tie_or_clone_weights',\n",
              " '_tied_weights_keys',\n",
              " '_update_model_kwargs_for_generation',\n",
              " '_upload_modified_files',\n",
              " '_validate_generated_length',\n",
              " '_validate_model_class',\n",
              " '_validate_model_kwargs',\n",
              " '_version',\n",
              " '_wrapped_call_impl',\n",
              " 'active_adapter',\n",
              " 'active_adapters',\n",
              " 'add_adapter',\n",
              " 'add_memory_hooks',\n",
              " 'add_model_tags',\n",
              " 'add_module',\n",
              " 'apply',\n",
              " 'base_model',\n",
              " 'base_model_prefix',\n",
              " 'bfloat16',\n",
              " 'buffers',\n",
              " 'call_super_init',\n",
              " 'can_generate',\n",
              " 'children',\n",
              " 'classifier',\n",
              " 'compile',\n",
              " 'compute_transition_scores',\n",
              " 'config',\n",
              " 'config_class',\n",
              " 'cpu',\n",
              " 'create_extended_attention_mask_for_decoder',\n",
              " 'cuda',\n",
              " 'dequantize',\n",
              " 'device',\n",
              " 'disable_adapters',\n",
              " 'disable_input_require_grads',\n",
              " 'double',\n",
              " 'dtype',\n",
              " 'dummy_inputs',\n",
              " 'dump_patches',\n",
              " 'enable_adapters',\n",
              " 'enable_input_require_grads',\n",
              " 'estimate_tokens',\n",
              " 'eval',\n",
              " 'extra_repr',\n",
              " 'float',\n",
              " 'floating_point_ops',\n",
              " 'forward',\n",
              " 'framework',\n",
              " 'from_pretrained',\n",
              " 'generate',\n",
              " 'generation_config',\n",
              " 'get_adapter_state_dict',\n",
              " 'get_buffer',\n",
              " 'get_extended_attention_mask',\n",
              " 'get_extra_state',\n",
              " 'get_head_mask',\n",
              " 'get_input_embeddings',\n",
              " 'get_memory_footprint',\n",
              " 'get_output_embeddings',\n",
              " 'get_parameter',\n",
              " 'get_position_embeddings',\n",
              " 'get_submodule',\n",
              " 'gradient_checkpointing_disable',\n",
              " 'gradient_checkpointing_enable',\n",
              " 'half',\n",
              " 'hf_device_map',\n",
              " 'init_weights',\n",
              " 'invert_attention_mask',\n",
              " 'ipu',\n",
              " 'is_gradient_checkpointing',\n",
              " 'is_parallelizable',\n",
              " 'load_adapter',\n",
              " 'load_state_dict',\n",
              " 'main_input_name',\n",
              " 'model_tags',\n",
              " 'modules',\n",
              " 'name_or_path',\n",
              " 'named_buffers',\n",
              " 'named_children',\n",
              " 'named_modules',\n",
              " 'named_parameters',\n",
              " 'num_labels',\n",
              " 'num_parameters',\n",
              " 'parameters',\n",
              " 'post_init',\n",
              " 'prepare_inputs_for_generation',\n",
              " 'prune_heads',\n",
              " 'push_to_hub',\n",
              " 'register_backward_hook',\n",
              " 'register_buffer',\n",
              " 'register_for_auto_class',\n",
              " 'register_forward_hook',\n",
              " 'register_forward_pre_hook',\n",
              " 'register_full_backward_hook',\n",
              " 'register_full_backward_pre_hook',\n",
              " 'register_load_state_dict_post_hook',\n",
              " 'register_module',\n",
              " 'register_parameter',\n",
              " 'register_state_dict_pre_hook',\n",
              " 'requires_grad_',\n",
              " 'reset_memory_hooks_state',\n",
              " 'resize_position_embeddings',\n",
              " 'resize_token_embeddings',\n",
              " 'retrieve_modules_from_names',\n",
              " 'reverse_bettertransformer',\n",
              " 'save_pretrained',\n",
              " 'segformer',\n",
              " 'set_adapter',\n",
              " 'set_extra_state',\n",
              " 'set_input_embeddings',\n",
              " 'share_memory',\n",
              " 'state_dict',\n",
              " 'supports_gradient_checkpointing',\n",
              " 'tie_weights',\n",
              " 'to',\n",
              " 'to_bettertransformer',\n",
              " 'to_empty',\n",
              " 'train',\n",
              " 'training',\n",
              " 'type',\n",
              " 'warn_if_padding_and_no_attention_mask',\n",
              " 'warnings_issued',\n",
              " 'xpu',\n",
              " 'zero_grad']"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get models memory of each layer"
      ],
      "metadata": {
        "id": "Eonr1_5_Y4eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
        "# model.state_dict()\n",
        "# https://stackoverflow.com/questions/54361763/pytorch-why-is-the-memory-occupied-by-the-tensor-variable-so-small\n",
        "# __sizeof__: Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to.\n",
        "# print(\"name, size, element_size, __sizeof__, is_quantized\")\n",
        "layers_mems = {}\n",
        "for m in models.keys():\n",
        "  layers = []\n",
        "  for name, param in models[m]['model'].named_parameters():\n",
        "    layers.append( (name.split(\".\")[-1], param.element_size() * param.numel()) )\n",
        "  layers_unique = set([i[0] for i in layers])\n",
        "  layers_mems[m] = {\n",
        "      luniq:sum([ layer[1] for layer in layers if layer[0] == luniq])\n",
        "      for luniq in layers_unique\n",
        "  }\n",
        "layers_mems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA6Fv4V0QChO",
        "outputId": "055d9c9d-9cfb-4e00-8de3-4ec8720ab631"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_base': {'bias': 75680, 'weight': 14229888},\n",
              " 'model_half': {'bias': 37840, 'weight': 7114944},\n",
              " 'model_int8': {'bias': 37840, 'weight': 4804800},\n",
              " 'model_int4': {'bias': 37840, 'weight': 3649728}}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print model stats"
      ],
      "metadata": {
        "id": "5m-YiKQ0cUG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in models.keys():\n",
        "  print(f\"sum({m}): {(layers_mems[m]['bias']+layers_mems[m]['weight'])/(2**10)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9sa1p3pdoMS",
        "outputId": "3a634643-7468-4249-ea51-06c59ee264eb"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum(model_base): 13970.28125\n",
            "sum(model_half): 6985.140625\n",
            "sum(model_int8): 4729.140625\n",
            "sum(model_int4): 3601.140625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgVK5mnq_utc",
        "outputId": "78a84480-325e-4923-cf05-3b504899fa79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name\t\tsize [MiB]\tratio\ttime\tdevice_map\n",
            "model_base\t13.64\t\t1.0\t1.43\t{'': device(type='cuda', index=0)}\n",
            "model_half\t6.82\t\t0.5\t1.21\t{'': device(type='cuda', index=0)}\n",
            "model_int8\t4.62\t\t0.34\t0.92\t{'': device(type='cuda', index=0)}\n",
            "model_int4\t3.52\t\t0.26\t1.36\t{'': device(type='cuda', index=0)}\n"
          ]
        }
      ],
      "source": [
        "print_models_stats(\n",
        "  models_stats, models_stats[\"model_base\"][\"memory_footprint\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1F4R3dncHb9"
      },
      "source": [
        "## Test model with example input\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "2_BgAFh1-q5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "222e152e-696b-4909-ea5b-94954ac8b057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "test_image = dataset['train'][0]['image']\n",
        "inputs = image_processor(images=test_image, return_tensors=\"pt\")\n",
        "inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(deviceinfo[\"devicename\"].type)\n",
        "inputs_halfed = inputs.copy()\n",
        "inputs_halfed[\"pixel_values\"] = inputs_halfed[\"pixel_values\"].half()\n",
        "test_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "8tWaGfoFdvHK"
      },
      "outputs": [],
      "source": [
        "def infer_model_input_print(\n",
        "    model_name: str,\n",
        "    model: SegformerForImageClassification,\n",
        "    input: dict\n",
        ") -> None:\n",
        "  print(model_name)\n",
        "  with no_grad():\n",
        "    outputs = model(**input)\n",
        "  pred_cls_idx = outputs.logits.argmax(-1).item()\n",
        "  pred_cls_lbl = model.config.id2label[pred_cls_idx]\n",
        "  print(f\"{outputs.keys()=}, {outputs.logits.shape=}\")\n",
        "  print(f\"{pred_cls_idx=}, {pred_cls_lbl=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for k in models.keys():\n",
        "#  input = inputs_halfed if models[k][\"model\"].is_quantized else inputs\n",
        "#  infer_model_input_print(k, models[k][\"model\"], input)"
      ],
      "metadata": {
        "id": "06RM1NxxmY8I"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7xdPbfja6fE"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Different amount of non-trainable params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "gQ7qOYr_ojnw",
        "outputId": "d94ee2c3-f7dd-4542-e826-92b144a7b6d4"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torchinfo.torchinfo.summary(model: 'nn.Module', input_size: 'INPUT_SIZE_TYPE | None' = None, input_data: 'INPUT_DATA_TYPE | None' = None, batch_dim: 'int | None' = None, cache_forward_pass: 'bool | None' = None, col_names: 'Iterable[str] | None' = None, col_width: 'int' = 25, depth: 'int' = 3, device: 'torch.device | str | None' = None, dtypes: 'list[torch.dtype] | None' = None, mode: 'str | None' = None, row_settings: 'Iterable[str] | None' = None, verbose: 'int | None' = None, **kwargs: 'Any') -> 'ModelStatistics'>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchinfo.torchinfo.summary</b><br/>def summary(model: nn.Module, input_size: INPUT_SIZE_TYPE | None=None, input_data: INPUT_DATA_TYPE | None=None, batch_dim: int | None=None, cache_forward_pass: bool | None=None, col_names: Iterable[str] | None=None, col_width: int=25, depth: int=3, device: torch.device | str | None=None, dtypes: list[torch.dtype] | None=None, mode: str | None=None, row_settings: Iterable[str] | None=None, verbose: int | None=None, **kwargs: Any) -&gt; ModelStatistics</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py</a>Summarize the given PyTorch model. Summarized information includes:\n",
              "    1) Layer names,\n",
              "    2) input/output shapes,\n",
              "    3) kernel shape,\n",
              "    4) # of parameters,\n",
              "    5) # of operations (Mult-Adds),\n",
              "    6) whether layer is trainable\n",
              "\n",
              "NOTE: If neither input_data or input_size are provided, no forward pass through the\n",
              "network is performed, and the provided model information is limited to layer names.\n",
              "\n",
              "Args:\n",
              "    model (nn.Module):\n",
              "            PyTorch model to summarize. The model should be fully in either train()\n",
              "            or eval() mode. If layers are not all in the same mode, running summary\n",
              "            may have side effects on batchnorm or dropout statistics. If you\n",
              "            encounter an issue with this, please open a GitHub issue.\n",
              "\n",
              "    input_size (Sequence of Sizes):\n",
              "            Shape of input data as a List/Tuple/torch.Size\n",
              "            (dtypes must match model input, default is FloatTensors).\n",
              "            You should include batch size in the tuple.\n",
              "            Default: None\n",
              "\n",
              "    input_data (Sequence of Tensors):\n",
              "            Arguments for the model&#x27;s forward pass (dtypes inferred).\n",
              "            If the forward() function takes several parameters, pass in a list of\n",
              "            args or a dict of kwargs (if your forward() function takes in a dict\n",
              "            as its only argument, wrap it in a list).\n",
              "            Default: None\n",
              "\n",
              "    batch_dim (int):\n",
              "            Batch_dimension of input data. If batch_dim is None, assume\n",
              "            input_data / input_size contains the batch dimension, which is used\n",
              "            in all calculations. Else, expand all tensors to contain the batch_dim.\n",
              "            Specifying batch_dim can be an runtime optimization, since if batch_dim\n",
              "            is specified, torchinfo uses a batch size of 1 for the forward pass.\n",
              "            Default: None\n",
              "\n",
              "    cache_forward_pass (bool):\n",
              "            If True, cache the run of the forward() function using the model\n",
              "            class name as the key. If the forward pass is an expensive operation,\n",
              "            this can make it easier to modify the formatting of your model\n",
              "            summary, e.g. changing the depth or enabled column types, especially\n",
              "            in Jupyter Notebooks.\n",
              "            WARNING: Modifying the model architecture or input data/input size when\n",
              "            this feature is enabled does not invalidate the cache or re-run the\n",
              "            forward pass, and can cause incorrect summaries as a result.\n",
              "            Default: False\n",
              "\n",
              "    col_names (Iterable[str]):\n",
              "            Specify which columns to show in the output. Currently supported: (\n",
              "                &quot;input_size&quot;,\n",
              "                &quot;output_size&quot;,\n",
              "                &quot;num_params&quot;,\n",
              "                &quot;params_percent&quot;,\n",
              "                &quot;kernel_size&quot;,\n",
              "                &quot;mult_adds&quot;,\n",
              "                &quot;trainable&quot;,\n",
              "            )\n",
              "            Default: (&quot;output_size&quot;, &quot;num_params&quot;)\n",
              "            If input_data / input_size are not provided, only &quot;num_params&quot; is used.\n",
              "\n",
              "    col_width (int):\n",
              "            Width of each column.\n",
              "            Default: 25\n",
              "\n",
              "    depth (int):\n",
              "            Depth of nested layers to display (e.g. Sequentials).\n",
              "            Nested layers below this depth will not be displayed in the summary.\n",
              "            Default: 3\n",
              "\n",
              "    device (torch.Device):\n",
              "            Uses this torch device for model and input_data.\n",
              "            If not specified, uses the dtype of input_data if given, or the\n",
              "            parameters of the model. Otherwise, uses the result of\n",
              "            torch.cuda.is_available().\n",
              "            Default: None\n",
              "\n",
              "    dtypes (List[torch.dtype]):\n",
              "            If you use input_size, torchinfo assumes your input uses FloatTensors.\n",
              "            If your model use a different data type, specify that dtype.\n",
              "            For multiple inputs, specify the size of both inputs, and\n",
              "            also specify the types of each parameter here.\n",
              "            Default: None\n",
              "\n",
              "    mode (str)\n",
              "            Either &quot;train&quot; or &quot;eval&quot;, which determines whether we call\n",
              "            model.train() or model.eval() before calling summary().\n",
              "            Default: &quot;eval&quot;.\n",
              "\n",
              "    row_settings (Iterable[str]):\n",
              "            Specify which features to show in a row. Currently supported: (\n",
              "                &quot;ascii_only&quot;,\n",
              "                &quot;depth&quot;,\n",
              "                &quot;var_names&quot;,\n",
              "            )\n",
              "            Default: (&quot;depth&quot;,)\n",
              "\n",
              "    verbose (int):\n",
              "            0 (quiet): No output\n",
              "            1 (default): Print model summary\n",
              "            2 (verbose): Show weight and bias layers in full detail\n",
              "            Default: 1\n",
              "            If using a Juypter Notebook or Google Colab, the default is 0.\n",
              "\n",
              "    **kwargs:\n",
              "            Other arguments used in `model.forward` function. Passing *args is no\n",
              "            longer supported.\n",
              "\n",
              "Return:\n",
              "    ModelStatistics object\n",
              "            See torchinfo/model_statistics.py for more information.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 52);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P1stJ4z9GLy",
        "outputId": "a419d082-aafc-4e00-fd17-88bb72deb7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model_base\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                                                      Param #\n",
            "====================================================================================================\n",
            "SegformerForImageClassification                                             --\n",
            "SegformerModel: 1-1                                                       --\n",
            "    SegformerEncoder: 2-1                                                --\n",
            "        ModuleList: 3-1                                                 485,472\n",
            "        ModuleList: 3-2                                                 2,832,896\n",
            "        ModuleList: 3-3                                                 1,024\n",
            "Linear: 1-2                                                               257,000\n",
            "====================================================================================================\n",
            "Total params: 3,576,392\n",
            "Trainable params: 3,576,392\n",
            "Non-trainable params: 0\n",
            "====================================================================================================\n",
            "\n",
            "model_half\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                                                      Param #\n",
            "====================================================================================================\n",
            "SegformerForImageClassification                                             --\n",
            "SegformerModel: 1-1                                                       --\n",
            "    SegformerEncoder: 2-1                                                --\n",
            "        ModuleList: 3-1                                                 485,472\n",
            "        ModuleList: 3-2                                                 2,832,896\n",
            "        ModuleList: 3-3                                                 1,024\n",
            "Linear: 1-2                                                               257,000\n",
            "====================================================================================================\n",
            "Total params: 3,576,392\n",
            "Trainable params: 3,576,392\n",
            "Non-trainable params: 0\n",
            "====================================================================================================\n",
            "\n",
            "model_int8\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                                                      Param #\n",
            "====================================================================================================\n",
            "SegformerForImageClassification                                             --\n",
            "SegformerModel: 1-1                                                       --\n",
            "    SegformerEncoder: 2-1                                                --\n",
            "        ModuleList: 3-1                                                 485,472\n",
            "        ModuleList: 3-2                                                 2,832,896\n",
            "        ModuleList: 3-3                                                 1,024\n",
            "Linear: 1-2                                                               257,000\n",
            "====================================================================================================\n",
            "Total params: 3,576,392\n",
            "Trainable params: 1,257,032\n",
            "Non-trainable params: 2,319,360\n",
            "====================================================================================================\n",
            "\n",
            "model_int4\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                                                      Param #\n",
            "====================================================================================================\n",
            "SegformerForImageClassification                                             --\n",
            "SegformerModel: 1-1                                                       --\n",
            "    SegformerEncoder: 2-1                                                --\n",
            "        ModuleList: 3-1                                                 485,472\n",
            "        ModuleList: 3-2                                                 1,677,824\n",
            "        ModuleList: 3-3                                                 1,024\n",
            "Linear: 1-2                                                               257,000\n",
            "====================================================================================================\n",
            "Total params: 2,421,320\n",
            "Trainable params: 1,257,032\n",
            "Non-trainable params: 1,164,288\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# shape 2d input (C, H, W), 1d (C, L)\n",
        "#TODO summary with input_size=(batch_size, 3, 512,512)) #, device='cpu')\n",
        "'''\n",
        "/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\n",
        "Summarize the given PyTorch model. Summarized information includes:\n",
        "    1) Layer names,\n",
        "    2) input/output shapes,\n",
        "    3) kernel shape,\n",
        "    4) # of parameters,\n",
        "    5) # of operations (Mult-Adds),\n",
        "    6) whether layer is trainable\n",
        "'''\n",
        "for m in models.keys():\n",
        "  sum = summary(models[m][\"model\"])\n",
        "  print(f\"\\n{m}\\n{sum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IapeDrLnZ6YJ"
      },
      "source": [
        "## Analysis of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "smyU5CowmzW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fed94ff-2276-4d71-c026-6ed72c5c71a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model_base\n",
            "base_model_prefix:\tsegformer\n",
            "call_super_init:\tFalse\n",
            "main_input_name:\tpixel_values\n",
            "classifier:\tLinear(in_features=256, out_features=1000, bias=True)\n",
            "num_labels:\t1000\n",
            "dump_patches:\tFalse\n",
            "is_parallelizable:\tFalse\n",
            "name_or_path:\tnvidia/mit-b0\n",
            "\n",
            "model_half\n",
            "base_model_prefix:\tsegformer\n",
            "call_super_init:\tFalse\n",
            "main_input_name:\tpixel_values\n",
            "classifier:\tLinear(in_features=256, out_features=1000, bias=True)\n",
            "num_labels:\t1000\n",
            "dump_patches:\tFalse\n",
            "is_parallelizable:\tFalse\n",
            "name_or_path:\tnvidia/mit-b0\n",
            "\n",
            "model_int8\n",
            "base_model_prefix:\tsegformer\n",
            "call_super_init:\tFalse\n",
            "main_input_name:\tpixel_values\n",
            "classifier:\tLinear(in_features=256, out_features=1000, bias=True)\n",
            "num_labels:\t1000\n",
            "dump_patches:\tFalse\n",
            "is_parallelizable:\tFalse\n",
            "name_or_path:\tnvidia/mit-b0\n",
            "\n",
            "model_int4\n",
            "base_model_prefix:\tsegformer\n",
            "call_super_init:\tFalse\n",
            "main_input_name:\tpixel_values\n",
            "classifier:\tLinear(in_features=256, out_features=1000, bias=True)\n",
            "num_labels:\t1000\n",
            "dump_patches:\tFalse\n",
            "is_parallelizable:\tFalse\n",
            "name_or_path:\tnvidia/mit-b0\n"
          ]
        }
      ],
      "source": [
        "features = (\n",
        "    \"base_model_prefix\", \"call_super_init\", \"main_input_name\", \"classifier\",\n",
        "    \"num_labels\", \"dump_patches\", \"is_parallelizable\", \"name_or_path\"\n",
        ")\n",
        "for m in models.keys():\n",
        "  print(f\"\\n{m}\")\n",
        "  for feature in features:\n",
        "    model = models[m][\"model\"]\n",
        "    f = eval(f\"model.{feature}\")\n",
        "    print(f\"{feature}:\\t{f}\")\n",
        "del(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_96rOGTczGs",
        "outputId": "70389dc4-608b-4e9d-af20-60f70b717699"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'config', 'name_or_path', 'warnings_issued', 'generation_config', '_keep_in_fp32_modules', 'num_labels', '_is_hf_initialized', 'hf_device_map'])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "models[\"model_base\"][\"model\"].__dict__.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWNfCDy9Uh_-"
      },
      "source": [
        "### Model graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get config: print(models[\"model_base\"][\"model\"]._modules[k].__dict__['config'])\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel\n",
        "'''\n",
        "TODO\n",
        "  named_parameters():\n",
        "  pass\n",
        "  #print(f\"{name}\")\n",
        "  #print(f\"\\t{param.size()}, {param.element_size()}, {param.__sizeof__()}, {param.is_quantized}\")\n",
        "  numel()*element_size()\n",
        "'''\n",
        "def print_model_structure(\n",
        "    modules, print_level=False, print_mem=False,\n",
        "    start_level=0, max_level=99, cur_level=0\n",
        "):\n",
        "  '''TODO'''\n",
        "  assert start_level <= max_level\n",
        "  assert start_level <= cur_level\n",
        "  if cur_level >= max_level: return\n",
        "  lvl = f\"{cur_level} \" if print_level else \"\"\n",
        "  if start_level != cur_level:\n",
        "    t = \" \".join(\" \" * cur_level)\n",
        "    nl = f\"{t} \"\n",
        "  else:\n",
        "    nl = \"- \"\n",
        "  for k in modules:\n",
        "    print(f\"{nl}{lvl}{k}\")\n",
        "    print_model_structure(\n",
        "        modules[k]._modules, print_level, print_mem,\n",
        "        start_level, max_level, cur_level+1\n",
        "    )"
      ],
      "metadata": {
        "id": "8xN4PULvYGVV"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modules = models[\"model_base\"][\"model\"]._modules\n",
        "print_model_structure(modules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGQpwU32Yf4u",
        "outputId": "c324966d-3998-41bf-ada0-b661e1009211"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- segformer\n",
            "  encoder\n",
            "    patch_embeddings\n",
            "      0\n",
            "        proj\n",
            "        layer_norm\n",
            "      1\n",
            "        proj\n",
            "        layer_norm\n",
            "      2\n",
            "        proj\n",
            "        layer_norm\n",
            "      3\n",
            "        proj\n",
            "        layer_norm\n",
            "    block\n",
            "      0\n",
            "        0\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "        1\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "      1\n",
            "        0\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "        1\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "      2\n",
            "        0\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "        1\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "              sr\n",
            "              layer_norm\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "      3\n",
            "        0\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "        1\n",
            "          layer_norm_1\n",
            "          attention\n",
            "            self\n",
            "              query\n",
            "              key\n",
            "              value\n",
            "              dropout\n",
            "            output\n",
            "              dense\n",
            "              dropout\n",
            "          drop_path\n",
            "          layer_norm_2\n",
            "          mlp\n",
            "            dense1\n",
            "            dwconv\n",
            "              dwconv\n",
            "            intermediate_act_fn\n",
            "            dense2\n",
            "            dropout\n",
            "    layer_norm\n",
            "      0\n",
            "      1\n",
            "      2\n",
            "      3\n",
            "- classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpXPFjLBb_rc"
      },
      "source": [
        "### Encoder `config` attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V_Taw0JC6Pk",
        "outputId": "008b7a6c-9f82-4d97-ac30-7778a4ba7100"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['return_dict', 'output_hidden_states', 'output_attentions', 'torchscript', 'torch_dtype', 'use_bfloat16', 'tf_legacy_loss', 'pruned_heads', 'tie_word_embeddings', 'chunk_size_feed_forward', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'num_beam_groups', 'diversity_penalty', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'task_specific_params', 'problem_type', '_name_or_path', '_commit_hash', '_attn_implementation_internal', 'transformers_version', 'downsampling_rates', 'image_size', 'model_type', 'num_channels', 'num_encoder_blocks', 'depths', 'sr_ratios', 'hidden_sizes', 'patch_sizes', 'strides', 'mlp_ratios', 'num_attention_heads', 'hidden_act', 'hidden_dropout_prob', 'attention_probs_dropout_prob', 'classifier_dropout_prob', 'initializer_range', 'drop_path_rate', 'layer_norm_eps', 'decoder_hidden_size', 'reshape_last_stage', 'semantic_loss_ignore_index'])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "models[\"model_base\"][\"model\"].base_model._modules[\"encoder\"].\\\n",
        "  config.__dict__.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IikYQF0dbRtT"
      },
      "source": [
        "### Encoder Attn W&B shapes and types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCGcQLX5dnhH",
        "outputId": "63c562db-0a50-439a-f1ff-3dcf2cbaf6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape, type, dtype\n",
            "model_base\n",
            "W: torch.Size([32, 32]), <class 'torch.nn.parameter.Parameter'>, torch.float32\n",
            "B: torch.Size([32]), <class 'torch.nn.parameter.Parameter'>, torch.float32\n",
            "model_half\n",
            "W: torch.Size([32, 32]), <class 'torch.nn.parameter.Parameter'>, torch.float16\n",
            "B: torch.Size([32]), <class 'torch.nn.parameter.Parameter'>, torch.float16\n",
            "model_int8\n",
            "W: torch.Size([32, 32]), <class 'bitsandbytes.nn.modules.Int8Params'>, torch.int8\n",
            "B: torch.Size([32]), <class 'torch.nn.parameter.Parameter'>, torch.float16\n",
            "model_int4\n",
            "W: torch.Size([512, 1]), <class 'bitsandbytes.nn.modules.Params4bit'>, torch.uint8\n",
            "B: torch.Size([32]), <class 'torch.nn.parameter.Parameter'>, torch.float16\n"
          ]
        }
      ],
      "source": [
        "print(\"shape, type, dtype\")\n",
        "for m in models.keys():\n",
        "  w = models[m][\"model\"].base_model.encoder.block[0][0].\\\n",
        "    attention.self.query.weight\n",
        "  b = models[m][\"model\"].base_model.encoder.block[0][0].\\\n",
        "    attention.self.query.bias\n",
        "  print(m)\n",
        "  print(f\"W: {w.shape}, {type(w)}, {w.dtype}\")\n",
        "  print(f\"B: {b.shape}, {type(b)}, {b.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX0p7gB7TgNF"
      },
      "source": [
        "### Encoder Attn W&B binarized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgKf79nFNm4C",
        "outputId": "a7aaaf56-eee0-4370-faa9-752a93dc9708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wandb={'weights': Parameter containing:\n",
            "tensor([[ 0.2056, -0.0214, -0.0157,  ..., -0.2073, -0.0188, -0.1008],\n",
            "        [ 0.0999, -0.0270,  0.2822,  ..., -0.5098, -0.0323, -0.0946],\n",
            "        [-0.0916, -0.1218, -0.1352,  ...,  0.0769, -0.1750, -0.1733],\n",
            "        ...,\n",
            "        [-0.2343, -0.2502, -0.2035,  ...,  0.3643,  0.2711, -0.1026],\n",
            "        [-0.1128,  0.2950,  0.0249,  ...,  0.1715,  0.0009, -0.0659],\n",
            "        [ 0.0486, -0.2077,  0.0285,  ...,  0.4297, -0.2127,  0.2493]],\n",
            "       device='cuda:0', requires_grad=True), 'biases': Parameter containing:\n",
            "tensor([-0.9663, -0.8828, -1.9095, -2.2925,  1.3038,  1.1518, -0.1643,  2.1551,\n",
            "         1.6874,  0.0306, -2.6529, -0.1244, -2.5922,  1.1711,  0.3846,  4.2415,\n",
            "         0.0925,  0.4346,  0.6358, -1.7323, -0.4298,  0.0603,  0.1595,  1.0717,\n",
            "        -0.9436, -0.7558,  3.4137, -0.4830, -2.7237, -3.5547,  4.3426, -1.5543],\n",
            "       device='cuda:0', requires_grad=True)}\n",
            "wandb_binarized={'weights': tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 1, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 0],\n",
            "        [0, 1, 1,  ..., 1, 1, 0],\n",
            "        [1, 0, 1,  ..., 1, 0, 1]], device='cuda:0'), 'biases': tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')}\n",
            "wandb_binarized_for_softmax={'weights': tensor([[ 1, -1, -1,  ..., -1, -1, -1],\n",
            "        [ 1, -1,  1,  ..., -1, -1, -1],\n",
            "        [-1, -1, -1,  ...,  1, -1, -1],\n",
            "        ...,\n",
            "        [-1, -1, -1,  ...,  1,  1, -1],\n",
            "        [-1,  1,  1,  ...,  1,  1, -1],\n",
            "        [ 1, -1,  1,  ...,  1, -1,  1]], device='cuda:0'), 'biases': tensor([-1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,\n",
            "         1, -1, -1,  1,  1,  1, -1, -1,  1, -1, -1, -1,  1, -1],\n",
            "       device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "# test on single block\n",
        "# https://pytorch.org/docs/stable/torch.html#math-operations\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html\n",
        "# https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\n",
        "wandb = {\n",
        "  \"weights\": models[\"model_base\"][\"model\"].base_model.encoder.\\\n",
        "    block[0][0].attention.self.query.weight,\n",
        "  \"biases\": models[\"model_base\"][\"model\"].base_model.encoder.\\\n",
        "    block[0][0].attention.self.query.bias\n",
        "}\n",
        "#TODO to torch.param and grad\n",
        "# RuntimeError: only Tensors of floating point dtype can require gradients\n",
        "wandb_binarized = {\n",
        "    k:(ge(wandb[k], zeros(wandb[k].shape).to(deviceinfo[\"devicename\"].type))*1)\n",
        "    for k in wandb.keys()\n",
        "}\n",
        "wandb_binarized_for_softmax = {\n",
        "    k:where(wandb_binarized[k].detach().clone() == 0, -1, 1)\n",
        "    for k in wandb.keys()\n",
        "}\n",
        "print(f\"{wandb=}\\n{wandb_binarized=}\\n{wandb_binarized_for_softmax=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQqyG_ptX10u",
        "outputId": "7f55bcb7-60f4-4ae9-a4fc-1a625381170f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.0.attention.self.query.weight',\n",
              " '0.0.attention.self.key.weight',\n",
              " '0.0.attention.self.value.weight',\n",
              " '0.1.attention.self.query.weight',\n",
              " '0.1.attention.self.key.weight',\n",
              " '0.1.attention.self.value.weight',\n",
              " '1.0.attention.self.query.weight',\n",
              " '1.0.attention.self.key.weight',\n",
              " '1.0.attention.self.value.weight',\n",
              " '1.1.attention.self.query.weight',\n",
              " '1.1.attention.self.key.weight',\n",
              " '1.1.attention.self.value.weight',\n",
              " '2.0.attention.self.query.weight',\n",
              " '2.0.attention.self.key.weight',\n",
              " '2.0.attention.self.value.weight',\n",
              " '2.1.attention.self.query.weight',\n",
              " '2.1.attention.self.key.weight',\n",
              " '2.1.attention.self.value.weight',\n",
              " '3.0.attention.self.query.weight',\n",
              " '3.0.attention.self.key.weight',\n",
              " '3.0.attention.self.value.weight',\n",
              " '3.1.attention.self.query.weight',\n",
              " '3.1.attention.self.key.weight',\n",
              " '3.1.attention.self.value.weight']"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "# prepare full attention binarization\n",
        "import re\n",
        "rex = re.compile(r'.*attention.self.(?:query|key|value).(?:weight)') #|bias)')\n",
        "wandb_attn = [\n",
        "  n\n",
        "  for n,p in models[\"model_base\"][\"model\"].base_model.\\\n",
        "    encoder.block.named_parameters()\n",
        "  if rex.findall(n)\n",
        "]\n",
        "wandb_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fip7C2ZqbaZ8"
      },
      "source": [
        "### Encoder `named_parameters` with `requires_grad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "68rKw__FGnq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f8d0ce-d254-471f-c4ca-8f3c5c332395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.patch_embeddings.0.proj.weight\n",
            "encoder.patch_embeddings.0.proj.bias\n",
            "encoder.patch_embeddings.0.layer_norm.weight\n",
            "encoder.patch_embeddings.0.layer_norm.bias\n",
            "encoder.patch_embeddings.1.proj.weight\n",
            "encoder.patch_embeddings.1.proj.bias\n",
            "encoder.patch_embeddings.1.layer_norm.weight\n",
            "encoder.patch_embeddings.1.layer_norm.bias\n",
            "encoder.patch_embeddings.2.proj.weight\n",
            "encoder.patch_embeddings.2.proj.bias\n",
            "encoder.patch_embeddings.2.layer_norm.weight\n",
            "encoder.patch_embeddings.2.layer_norm.bias\n",
            "encoder.patch_embeddings.3.proj.weight\n",
            "encoder.patch_embeddings.3.proj.bias\n",
            "encoder.patch_embeddings.3.layer_norm.weight\n",
            "encoder.patch_embeddings.3.layer_norm.bias\n",
            "encoder.block.0.0.layer_norm_1.weight\n",
            "encoder.block.0.0.layer_norm_1.bias\n",
            "encoder.block.0.0.attention.self.query.weight\n",
            "encoder.block.0.0.attention.self.query.bias\n",
            "encoder.block.0.0.attention.self.key.weight\n",
            "encoder.block.0.0.attention.self.key.bias\n",
            "encoder.block.0.0.attention.self.value.weight\n",
            "encoder.block.0.0.attention.self.value.bias\n",
            "encoder.block.0.0.attention.self.sr.weight\n",
            "encoder.block.0.0.attention.self.sr.bias\n",
            "encoder.block.0.0.attention.self.layer_norm.weight\n",
            "encoder.block.0.0.attention.self.layer_norm.bias\n",
            "encoder.block.0.0.attention.output.dense.weight\n",
            "encoder.block.0.0.attention.output.dense.bias\n",
            "encoder.block.0.0.layer_norm_2.weight\n",
            "encoder.block.0.0.layer_norm_2.bias\n",
            "encoder.block.0.0.mlp.dense1.weight\n",
            "encoder.block.0.0.mlp.dense1.bias\n",
            "encoder.block.0.0.mlp.dwconv.dwconv.weight\n",
            "encoder.block.0.0.mlp.dwconv.dwconv.bias\n",
            "encoder.block.0.0.mlp.dense2.weight\n",
            "encoder.block.0.0.mlp.dense2.bias\n",
            "encoder.block.0.1.layer_norm_1.weight\n",
            "encoder.block.0.1.layer_norm_1.bias\n",
            "encoder.block.0.1.attention.self.query.weight\n",
            "encoder.block.0.1.attention.self.query.bias\n",
            "encoder.block.0.1.attention.self.key.weight\n",
            "encoder.block.0.1.attention.self.key.bias\n",
            "encoder.block.0.1.attention.self.value.weight\n",
            "encoder.block.0.1.attention.self.value.bias\n",
            "encoder.block.0.1.attention.self.sr.weight\n",
            "encoder.block.0.1.attention.self.sr.bias\n",
            "encoder.block.0.1.attention.self.layer_norm.weight\n",
            "encoder.block.0.1.attention.self.layer_norm.bias\n",
            "encoder.block.0.1.attention.output.dense.weight\n",
            "encoder.block.0.1.attention.output.dense.bias\n",
            "encoder.block.0.1.layer_norm_2.weight\n",
            "encoder.block.0.1.layer_norm_2.bias\n",
            "encoder.block.0.1.mlp.dense1.weight\n",
            "encoder.block.0.1.mlp.dense1.bias\n",
            "encoder.block.0.1.mlp.dwconv.dwconv.weight\n",
            "encoder.block.0.1.mlp.dwconv.dwconv.bias\n",
            "encoder.block.0.1.mlp.dense2.weight\n",
            "encoder.block.0.1.mlp.dense2.bias\n",
            "encoder.block.1.0.layer_norm_1.weight\n",
            "encoder.block.1.0.layer_norm_1.bias\n",
            "encoder.block.1.0.attention.self.query.weight\n",
            "encoder.block.1.0.attention.self.query.bias\n",
            "encoder.block.1.0.attention.self.key.weight\n",
            "encoder.block.1.0.attention.self.key.bias\n",
            "encoder.block.1.0.attention.self.value.weight\n",
            "encoder.block.1.0.attention.self.value.bias\n",
            "encoder.block.1.0.attention.self.sr.weight\n",
            "encoder.block.1.0.attention.self.sr.bias\n",
            "encoder.block.1.0.attention.self.layer_norm.weight\n",
            "encoder.block.1.0.attention.self.layer_norm.bias\n",
            "encoder.block.1.0.attention.output.dense.weight\n",
            "encoder.block.1.0.attention.output.dense.bias\n",
            "encoder.block.1.0.layer_norm_2.weight\n",
            "encoder.block.1.0.layer_norm_2.bias\n",
            "encoder.block.1.0.mlp.dense1.weight\n",
            "encoder.block.1.0.mlp.dense1.bias\n",
            "encoder.block.1.0.mlp.dwconv.dwconv.weight\n",
            "encoder.block.1.0.mlp.dwconv.dwconv.bias\n",
            "encoder.block.1.0.mlp.dense2.weight\n",
            "encoder.block.1.0.mlp.dense2.bias\n",
            "encoder.block.1.1.layer_norm_1.weight\n",
            "encoder.block.1.1.layer_norm_1.bias\n",
            "encoder.block.1.1.attention.self.query.weight\n",
            "encoder.block.1.1.attention.self.query.bias\n",
            "encoder.block.1.1.attention.self.key.weight\n",
            "encoder.block.1.1.attention.self.key.bias\n",
            "encoder.block.1.1.attention.self.value.weight\n",
            "encoder.block.1.1.attention.self.value.bias\n",
            "encoder.block.1.1.attention.self.sr.weight\n",
            "encoder.block.1.1.attention.self.sr.bias\n",
            "encoder.block.1.1.attention.self.layer_norm.weight\n",
            "encoder.block.1.1.attention.self.layer_norm.bias\n",
            "encoder.block.1.1.attention.output.dense.weight\n",
            "encoder.block.1.1.attention.output.dense.bias\n",
            "encoder.block.1.1.layer_norm_2.weight\n",
            "encoder.block.1.1.layer_norm_2.bias\n",
            "encoder.block.1.1.mlp.dense1.weight\n",
            "encoder.block.1.1.mlp.dense1.bias\n",
            "encoder.block.1.1.mlp.dwconv.dwconv.weight\n",
            "encoder.block.1.1.mlp.dwconv.dwconv.bias\n",
            "encoder.block.1.1.mlp.dense2.weight\n",
            "encoder.block.1.1.mlp.dense2.bias\n",
            "encoder.block.2.0.layer_norm_1.weight\n",
            "encoder.block.2.0.layer_norm_1.bias\n",
            "encoder.block.2.0.attention.self.query.weight\n",
            "encoder.block.2.0.attention.self.query.bias\n",
            "encoder.block.2.0.attention.self.key.weight\n",
            "encoder.block.2.0.attention.self.key.bias\n",
            "encoder.block.2.0.attention.self.value.weight\n",
            "encoder.block.2.0.attention.self.value.bias\n",
            "encoder.block.2.0.attention.self.sr.weight\n",
            "encoder.block.2.0.attention.self.sr.bias\n",
            "encoder.block.2.0.attention.self.layer_norm.weight\n",
            "encoder.block.2.0.attention.self.layer_norm.bias\n",
            "encoder.block.2.0.attention.output.dense.weight\n",
            "encoder.block.2.0.attention.output.dense.bias\n",
            "encoder.block.2.0.layer_norm_2.weight\n",
            "encoder.block.2.0.layer_norm_2.bias\n",
            "encoder.block.2.0.mlp.dense1.weight\n",
            "encoder.block.2.0.mlp.dense1.bias\n",
            "encoder.block.2.0.mlp.dwconv.dwconv.weight\n",
            "encoder.block.2.0.mlp.dwconv.dwconv.bias\n",
            "encoder.block.2.0.mlp.dense2.weight\n",
            "encoder.block.2.0.mlp.dense2.bias\n",
            "encoder.block.2.1.layer_norm_1.weight\n",
            "encoder.block.2.1.layer_norm_1.bias\n",
            "encoder.block.2.1.attention.self.query.weight\n",
            "encoder.block.2.1.attention.self.query.bias\n",
            "encoder.block.2.1.attention.self.key.weight\n",
            "encoder.block.2.1.attention.self.key.bias\n",
            "encoder.block.2.1.attention.self.value.weight\n",
            "encoder.block.2.1.attention.self.value.bias\n",
            "encoder.block.2.1.attention.self.sr.weight\n",
            "encoder.block.2.1.attention.self.sr.bias\n",
            "encoder.block.2.1.attention.self.layer_norm.weight\n",
            "encoder.block.2.1.attention.self.layer_norm.bias\n",
            "encoder.block.2.1.attention.output.dense.weight\n",
            "encoder.block.2.1.attention.output.dense.bias\n",
            "encoder.block.2.1.layer_norm_2.weight\n",
            "encoder.block.2.1.layer_norm_2.bias\n",
            "encoder.block.2.1.mlp.dense1.weight\n",
            "encoder.block.2.1.mlp.dense1.bias\n",
            "encoder.block.2.1.mlp.dwconv.dwconv.weight\n",
            "encoder.block.2.1.mlp.dwconv.dwconv.bias\n",
            "encoder.block.2.1.mlp.dense2.weight\n",
            "encoder.block.2.1.mlp.dense2.bias\n",
            "encoder.block.3.0.layer_norm_1.weight\n",
            "encoder.block.3.0.layer_norm_1.bias\n",
            "encoder.block.3.0.attention.self.query.weight\n",
            "encoder.block.3.0.attention.self.query.bias\n",
            "encoder.block.3.0.attention.self.key.weight\n",
            "encoder.block.3.0.attention.self.key.bias\n",
            "encoder.block.3.0.attention.self.value.weight\n",
            "encoder.block.3.0.attention.self.value.bias\n",
            "encoder.block.3.0.attention.output.dense.weight\n",
            "encoder.block.3.0.attention.output.dense.bias\n",
            "encoder.block.3.0.layer_norm_2.weight\n",
            "encoder.block.3.0.layer_norm_2.bias\n",
            "encoder.block.3.0.mlp.dense1.weight\n",
            "encoder.block.3.0.mlp.dense1.bias\n",
            "encoder.block.3.0.mlp.dwconv.dwconv.weight\n",
            "encoder.block.3.0.mlp.dwconv.dwconv.bias\n",
            "encoder.block.3.0.mlp.dense2.weight\n",
            "encoder.block.3.0.mlp.dense2.bias\n",
            "encoder.block.3.1.layer_norm_1.weight\n",
            "encoder.block.3.1.layer_norm_1.bias\n",
            "encoder.block.3.1.attention.self.query.weight\n",
            "encoder.block.3.1.attention.self.query.bias\n",
            "encoder.block.3.1.attention.self.key.weight\n",
            "encoder.block.3.1.attention.self.key.bias\n",
            "encoder.block.3.1.attention.self.value.weight\n",
            "encoder.block.3.1.attention.self.value.bias\n",
            "encoder.block.3.1.attention.output.dense.weight\n",
            "encoder.block.3.1.attention.output.dense.bias\n",
            "encoder.block.3.1.layer_norm_2.weight\n",
            "encoder.block.3.1.layer_norm_2.bias\n",
            "encoder.block.3.1.mlp.dense1.weight\n",
            "encoder.block.3.1.mlp.dense1.bias\n",
            "encoder.block.3.1.mlp.dwconv.dwconv.weight\n",
            "encoder.block.3.1.mlp.dwconv.dwconv.bias\n",
            "encoder.block.3.1.mlp.dense2.weight\n",
            "encoder.block.3.1.mlp.dense2.bias\n",
            "encoder.layer_norm.0.weight\n",
            "encoder.layer_norm.0.bias\n",
            "encoder.layer_norm.1.weight\n",
            "encoder.layer_norm.1.bias\n",
            "encoder.layer_norm.2.weight\n",
            "encoder.layer_norm.2.bias\n",
            "encoder.layer_norm.3.weight\n",
            "encoder.layer_norm.3.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in models[\"model_base\"][\"model\"].base_model.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeUyKhdNbwUH"
      },
      "source": [
        "### Encoder `patch_embeddings` attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH_7zFlNGIC-",
        "outputId": "e6c426ed-4b2e-48ee-e017-a8471c33f143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', '_is_hf_initialized'])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "models[\"model_base\"][\"model\"].base_model._modules['encoder'].\\\n",
        "  _modules['patch_embeddings'].__dict__.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-beBsFFiheD6"
      },
      "source": [
        "### Encoder `main_input_name`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6ta2BQuBhYDi",
        "outputId": "02eb5c7f-ad44-49b6-dbc6-cc0489c7eaee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pixel_values'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "models[\"model_base\"][\"model\"].main_input_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoy5irbWl2w0"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-uZMH1Nl6yi",
        "outputId": "8dcd63d6-f936-4305-c3ff-9b2809f4a2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No decoder found.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  models[\"model_base\"][\"model\"].decoder\n",
        "except:\n",
        "  print(\"No decoder found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOXmyPQ76Qv9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0PqjzHQVutb"
      },
      "source": [
        "## Collation and dtype halfing of batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "u0WcwsX7rW9w"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples: dict, half_data: bool = False) -> dict:\n",
        "    pixel_values = stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = tensor([example[\"label\"] for example in examples])\n",
        "    if (\"trainer\" in globals() and trainer.model.is_quantized) or half_data:\n",
        "      pixel_values = pixel_values.half() # to fit float16 biases\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8mxTKx7D8Dc",
        "outputId": "ce530688-ee20-4eaf-dcb7-74fe1576444d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ds_quantized.keys()=dict_keys(['pixel_values', 'labels'])\n",
            "ds_quantized['pixel_values'].dtype=torch.float16\n"
          ]
        }
      ],
      "source": [
        "# test collation\n",
        "ds_quantized = collate_fn(dataset[\"train\"].shard(10000, 0), True)\n",
        "print(f\"{ds_quantized.keys()=}\\n{ds_quantized['pixel_values'].dtype=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYMu0PlyUpnC"
      },
      "source": [
        "## Training as QAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "hEknhykz2nkl"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/datasets/v1.1.1/processing.html\n",
        "ds_sharding = (ds_num_shards, randrange(ds_num_shards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "fZsmKE5i-llh"
      },
      "outputs": [],
      "source": [
        "def add_args_and_trainer_to_model(model_name: str) -> None:\n",
        "  \"\"\"TODO\"\"\"\n",
        "  # if model_name == \"model_half\": pass\n",
        "  print(f\"Adding TrainingArguments and Trainer to '{model_name}'.\")\n",
        "  models[model_name][\"TrainArgs\"] = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-{dataset_name}\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_to_load\n",
        "    # push_to_hub=True,\n",
        "  )\n",
        "  models[model_name][\"Trainer\"] = Trainer(\n",
        "    models[model_name][\"model\"],\n",
        "    models[model_name][\"TrainArgs\"],\n",
        "    train_dataset=dataset[\"train\"].shard(*ds_sharding),\n",
        "    eval_dataset=dataset[\"test\"].shard(*ds_sharding),\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        "    # optimizers=(AdamW,'')\n",
        "  )\n",
        "\n",
        "def train_model(model_name: str) -> None:\n",
        "  \"\"\"TODO\"\"\"\n",
        "  print(f\"Training '{model_name}'.\")\n",
        "  trainer = models[model_name][\"Trainer\"]\n",
        "  models[model_name][\"TrainResults\"] = trainer.train()\n",
        "  metrics = models[model_name][\"TrainResults\"].metrics\n",
        "  # trainer.save_model()\n",
        "  trainer.log_metrics(\"train\", metrics)\n",
        "  trainer.save_metrics(\"train\", metrics)\n",
        "  models[model_name][\"EvalResults\"] = trainer.evaluate()\n",
        "  metrics = models[model_name][\"EvalResults\"]\n",
        "  trainer.log_metrics(\"eval\", metrics)\n",
        "  trainer.save_metrics(\"eval\", metrics)\n",
        "  # trainer.save_state()\n",
        "  # trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "Pps61vF_4QaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "83de0649-f8b1-407b-c019-c1f8a9f43edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding TrainingArguments and Trainer to 'model_base'.\n",
            "Training 'model_base'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:15, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.370507</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.752339</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.283700</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.958632</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.647328</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.420584</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.287985</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.208805</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.194303</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.268100</td>\n",
              "      <td>6.195221</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  total_flos               =  3771975GF\n",
            "  train_loss               =     1.2681\n",
            "  train_runtime            = 0:00:16.73\n",
            "  train_samples_per_second =     14.337\n",
            "  train_steps_per_second   =      0.597\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_accuracy           =        0.0\n",
            "  eval_loss               =     8.3705\n",
            "  eval_runtime            = 0:00:00.07\n",
            "  eval_samples_per_second =     27.056\n",
            "  eval_steps_per_second   =     13.528\n",
            "model_half\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-2eaaf7d14004>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodels_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_quantized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0madd_args_and_trainer_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''\n",
        "if trained on quantized:\n",
        "  ValueError: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details\n",
        "'''\n",
        "for k in models.keys():\n",
        "  # TODO peft\n",
        "  if models_stats[k]['is_quantized'] is not None:\n",
        "    print(k)\n",
        "    raise NotImplementedError\n",
        "  add_args_and_trainer_to_model(k)\n",
        "  train_model(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer"
      ],
      "metadata": {
        "id": "kcF2cLYkoUc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in models.keys():\n",
        "  input = inputs_halfed if models_stats[k]['is_quantized'] is not None else inputs\n",
        "  infer_model_input_print(k, models[k][\"model\"], input)\n",
        "  #print(models[k].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9AVagAnnOE0",
        "outputId": "29033fb5-e021-4220-d561-d366fe99e9b6"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_base\n",
            "outputs.keys()=odict_keys(['logits']), outputs.logits.shape=torch.Size([1, 1000])\n",
            "pred_cls_idx=701, pred_cls_lbl='parachute, chute'\n",
            "model_half\n",
            "outputs.keys()=odict_keys(['logits']), outputs.logits.shape=torch.Size([1, 1000])\n",
            "pred_cls_idx=701, pred_cls_lbl='parachute, chute'\n",
            "model_int8\n",
            "outputs.keys()=odict_keys(['logits']), outputs.logits.shape=torch.Size([1, 1000])\n",
            "pred_cls_idx=398, pred_cls_lbl='abacus'\n",
            "model_int4\n",
            "outputs.keys()=odict_keys(['logits']), outputs.logits.shape=torch.Size([1, 1000])\n",
            "pred_cls_idx=149, pred_cls_lbl='dugong, Dugong dugon'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
